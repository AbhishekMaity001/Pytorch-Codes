{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on Train Data!!... \n",
      "Accuracy on training set: 83.368340\n",
      " Checking accuracy on Test Data!!  \n",
      "Accuracy on test set: 84.25\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters setting\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "num_layers = 2\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # Set initial Hidden State\n",
    "        \n",
    "        # Forward Prop\n",
    "        out, _ = self.rnn(x, h0) # Every example will have their own Hidden state so we will ignore that and take only output\n",
    "        out = out.reshape(out.shape[0], -1) # Reshaping.. 28 x 256 seqlength x hidden_size\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "# Load the Data Train & Test\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "# Initialize the Network\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Network\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        \n",
    "        # Convert the data to the CUDA for faster training\n",
    "        data = data.to(device=device).squeeze(1) # bcoz MNIST has 1x28x28 but RNN expects N x 28 x 28 ---.squeeze(1) will remove 1 from that particular axis\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        # Flattening the data\n",
    "#         data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad() # so that they dont store the gradrients\n",
    "        loss.backward() # gradients computed here\n",
    "        \n",
    "        # gradient steps \n",
    "        optimizer.step() # here we update the weights based on the gradients computed on top loss.backward()\n",
    "        \n",
    "        \n",
    "# Check Accuracy on training & test set \n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on Train Data!!... \")\n",
    "    else:\n",
    "        print(\" Checking accuracy on Test Data!!  \")\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # let the model know that it is in evaluation mode\n",
    "    \n",
    "    # to let the model know that you dont have to compute the gradients while doing the tesing/accuracy checking\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device).squeeze(1)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "             # x = x.reshape(x.shape[0], -1) # Flattening\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        # print('{}/{} with accuracy {}'.format(num_correct, num_samples, (num_correct/num_samples)*100, 2))\n",
    "        \n",
    "    model.train()\n",
    "    return num_correct/num_samples\n",
    "    \n",
    "\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on Train Data!!... \n",
      "Accuracy on training set: 94.854996\n",
      " Checking accuracy on Test Data!!  \n",
      "Accuracy on test set: 94.63\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Hyperparameters setting\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "num_layers = 2\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True) # Change here to GRU\n",
    "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # Set initial Hidden State\n",
    "        \n",
    "        # Forward Prop\n",
    "        out, _ = self.gru(x, h0) # Every example will have their own Hidden state so we will ignore that and take only output\n",
    "        out = out.reshape(out.shape[0], -1) # Reshaping.. 28 x 256 seqlength x hidden_size\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Load the Data Train & Test\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "# Initialize the Network\n",
    "model = GRU(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Network\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        \n",
    "        # Convert the data to the CUDA for faster training\n",
    "        data = data.to(device=device).squeeze(1) # bcoz MNIST has 1x28x28 but RNN expects N x 28 x 28 ---.squeeze(1) will remove 1 from that particular axis\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        # Flattening the data\n",
    "#         data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad() # so that they dont store the gradrients\n",
    "        loss.backward() # gradients computed here\n",
    "        \n",
    "        # gradient steps \n",
    "        optimizer.step() # here we update the weights based on the gradients computed on top loss.backward()\n",
    "        \n",
    "        \n",
    "# Check Accuracy on training & test set \n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on Train Data!!... \")\n",
    "    else:\n",
    "        print(\" Checking accuracy on Test Data!!  \")\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # let the model know that it is in evaluation mode\n",
    "    \n",
    "    # to let the model know that you dont have to compute the gradients while doing the tesing/accuracy checking\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device).squeeze(1)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "             # x = x.reshape(x.shape[0], -1) # Flattening\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        # print('{}/{} with accuracy {}'.format(num_correct, num_samples, (num_correct/num_samples)*100, 2))\n",
    "        \n",
    "    model.train()\n",
    "    return num_correct/num_samples\n",
    "    \n",
    "\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on Train Data!!... \n",
      "Accuracy on training set: 98.313332\n",
      " Checking accuracy on Test Data!!  \n",
      "Accuracy on test set: 97.99\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Hyperparameters setting\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "num_layers = 2\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) # Change here to LSTM\n",
    "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # Set initial Hidden State \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # Set initial Cell State\n",
    "        \n",
    "        # Forward Prop # PASS BOTH THE CELL STATE AND HIDDEN STATE IN THIS FORWARD PROP\n",
    "        out, _ = self.gru(x, (h0, c0)) # Every example will have their own Hidden state so we will ignore that and take only output\n",
    "        # _ will here output the (cell_state, hidden_state)\n",
    "        out = out.reshape(out.shape[0], -1) # Reshaping.. 28 x 256 seqlength x hidden_size\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Load the Data Train & Test\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "# Initialize the Network\n",
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Network\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        \n",
    "        # Convert the data to the CUDA for faster training\n",
    "        data = data.to(device=device).squeeze(1) # bcoz MNIST has 1x28x28 but RNN expects N x 28 x 28 ---.squeeze(1) will remove 1 from that particular axis\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "        # Flattening the data\n",
    "#         data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad() # so that they dont store the gradrients\n",
    "        loss.backward() # gradients computed here\n",
    "        \n",
    "        # gradient steps \n",
    "        optimizer.step() # here we update the weights based on the gradients computed on top loss.backward()\n",
    "        \n",
    "        \n",
    "# Check Accuracy on training & test set \n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on Train Data!!... \")\n",
    "    else:\n",
    "        print(\" Checking accuracy on Test Data!!  \")\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # let the model know that it is in evaluation mode\n",
    "    \n",
    "    # to let the model know that you dont have to compute the gradients while doing the tesing/accuracy checking\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device).squeeze(1)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "             # x = x.reshape(x.shape[0], -1) # Flattening\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        # print('{}/{} with accuracy {}'.format(num_correct, num_samples, (num_correct/num_samples)*100, 2))\n",
    "        \n",
    "    model.train()\n",
    "    return num_correct/num_samples\n",
    "    \n",
    "\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
